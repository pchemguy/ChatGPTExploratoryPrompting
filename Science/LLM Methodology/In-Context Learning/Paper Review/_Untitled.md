Large Language Models (LLMs) are increasingly explored for complex analytical tasks within scientific domains. Our prior work introduced Persistent Workflow Prompting (PWP), a prompt engineering methodology intended to guide LLMs through intricate, multi-step analyses using standard chat interfaces [1]. 

Within the PWP-based _PeerReviewPrompt_, developed as an initial proof-of-concept for critically reviewing experimental chemistry manuscripts, LLM context conditioning technique played a significant role.
This technique, embedded in the prompt (e.g., via persona engineering and explicit operational directives), aimed to instill a critical analytical mindset and mitigate biases such as input bias during the evaluation of a single, deliberately chosen _test paper_ [1, 2].

The _PeerReviewPrompt_ focused on a potentially ill-defined analytical object (the "core methodology") within a relatively localized scope, where PWP-guided LLMs showed an ability to identify relevant text sections.

The present proof-of-concept study continues the investigation of LLM context conditioning, applying these principles to a distinct validation challenge: the identification of errors in chemical formulas within scientific manuscripts, including those in raster images (multimodal validation). This task presents a contrasting scenario: the analytical object (a chemical formula) is comparatively well-defined, but the scope of analysis must encompass the entire manuscript, posing a "needle in a haystack" problem for detecting subtle errors. Furthermore, our initial explorations suggested a critical LLM behavior: their inherent tendency to perform error correction and interpret user intent can lead them to overlook or silently "correct" the very inaccuracies targeted for detection. This tendency presents a specific challenge for validation tasks.

To explore this issue, this study tested several prompting strategies on the same _test paper_ [2] used previously, which is known to contain specific textual and image-based chemical formula errors. We began with basic direct prompts, moved to structured approaches incorporating task decomposition [3, 4] and self-reflection elements [5–7], and ultimately adapted the PWP architecture and context conditioning techniques from the _PeerReviewPrompt_ [1] to create a specialized _ChemicalFormulasValidationPrompt_. This paper details these exploratory approaches and reports on their observed performance. We analyze LLM behaviors—such as apparent error suppression, inconsistent effort ("laziness"), and hallucinations—and discuss how targeted context conditioning appeared to improve the reliability of chemical formula identification in our test case. Notably, observations with Gemini 2.5 Pro indicated successful multimodal error identification, an outcome not observed with ChatGPT Plus o3 under the same test conditions.

The remainder of this paper is organized as follows: Section 2 describes the methodology, including the single test case, the specific formula errors targeted, and the various prompting strategies explored. Section 3 presents the results and discussion, offering insights into observed LLM processing patterns and the apparent efficacy of context conditioning based on this specific test case. Finally, Section 4 provides concluding remarks and discusses potential avenues for future research.