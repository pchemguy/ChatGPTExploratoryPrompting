Here are full relevant sections to work on. Presently, I see three main tasks (all of which appear to be of broad importance that spans far beyond the present project's scope) I encountered when started this project:
1. Tacit expert knowledge codification
2. Development of generalized workflow for prompt that would guide AI analysis.
3. Countering input biases
While current intro identifies several difficulties that should be mentioned, there is no articulated mentioning of any of the tasks above. In fact, perhaps there is some linking to be made. The complexity of the problem, lack of training data and limitations of existing approaches discussed in the second half of the second para of 1.1 can be linked to the necessity to develop detailed workflow (task 2). Need for field specific tailoring - expert knowledge codification (task 1). Or do I have the links in Section 1.4, where the appropriate place is for linking general issues, specific articulated tasks, and proposed/developed solutions (or should linking be done in 1.2)?

I do not discuss the input biases in 1.1 at all, as I only recently articulated this matter. And this point is not identified as one of the paper's contributions in 1.4, but this matter possibly deserves a dedicated bullet point.

---
INTO NOTES FOR INPUT BIAS

The simulated reasoning feature is important for complex tasks like in-depth analysis of academic manuscripts. Reasoning deficiencies, such as biases, affect both humans and LLMs. Reasoning biases form a group of reasoning problems of varying nature, with particular attention given in this work to the *input bias* phenomenon, discussed both in context of human [[#]] and LLM [[#]] reasoning.

---

### 1.1. Scholarly Peer Review

Scholarly peer review is a cornerstone of academic research, demanding significant time, domain expertise, and critical reasoning. Using technical means to facilitate this process is a long-standing goal, which has gained urgency with the explosive growth of publishing activities and the recent advances in generative AI technologies increasingly used in academic publishing [[36, 37]]. The last few years alone have witnessed a wealth of publications addressing this automation problem via diverse approaches, including basic and methodological research [[38–46]], graph-based manuscript modeling [[47]], prompt-focused techniques [[42, 46]], probing capabilities of private and open-source models [[38, 48, 49]], investigations with reasoning models [[40, 50]], training custom models [[49–51]], developing multi-model/agentic systems [[38, 47–49, 52, 53]], and launching publicly accessible services [[38, 43, 50, 54]]. Due to its intellectually demanding nature, using AI for peer-review-like feedback also serves as a valuable method for evaluating and pushing the boundaries of advanced models.

Despite this progress, automating peer review remains a significant challenge for modern AI [[39–41, 55]]. Key difficulties include inherent complexity of this task, the need for field-specific tailoring, and the historical lack of readily available, large-scale training datasets (with a number of attempts to address the latter issue [[43, 45, 48–50, 56–63]]). Furthermore, existing approaches often face limitations. Training data consisting of high-level reviewer comments may not effectively teach models the detailed, step-by-step reasoning required for rigorous manuscript evaluation. Similarly, prompts based solely on common reviewer guideline questions (e.g., [[64]]) may fail to elicit the necessary depth of analysis compared to methods like chain-of-thought (CoT) prompting [[65–67]].

### 1.2. Our Approach: Persistent Workflow Prompting

To address these limitations, particularly the need for detailed procedural guidance in AI-driven review, we explore an approach centered on advanced prompt engineering. Instead of relying solely on ICL examples or simple question lists, we focus on codifying the intellectual workflow inherent in rigorous peer review. Drawing inspiration from techniques like least-to-most prompting [[68]], task decomposition [[69]], plan-and-solve prompting [[70]], role-playing [[71, 72]], and PC-SubQ [[73]], we introduce Persistent Workflow Prompting (PWP). PWP utilizes a highly structured, hierarchical prompt that guides an LLM through a detailed analysis process. This guidance involves decomposing the complex task of reviewing (specifically for experimental chemistry manuscripts in this work) into a sequence of manageable steps, effectively translating tacit expert knowledge [[74]] into an actionable protocol for the AI. This methodology allows for complex analysis using only the standard chat interface of LLMs.

### 1.4 Contributions and Outline

While this paper details complex and abstract methodologies, it also provides readily accessible materials designed to facilitate understanding and quick replication via generally available AI chat bots. Key resources, including the full Markdown-formatted **_PeerReviewPrompt_** text for use with LLMs, a file with the **_test paper_** (including SI), **usage protocol**, and **demonstration analyses** are available in the **Supporting Information**, allowing readers to quickly test and verify the core PWP application described herein (**primary target model is** **Gemini Advanced 2.5 Pro**).

The main contributions of this paper are:
1. **Persistent Workflow Prompting (PWP):** We design, implement, and **introduce PWP**, a prompt engineering methodology employing a persistent, structured, workflow-based prompt to guide LLMs through complex, multi-step analytical tasks via standard chat interfaces.
2. **PWP Prompt for Chemistry Review:** We present a proof-of-concept **PWP prompt** specifically designed for the critical analysis of experimental chemistry manuscripts, demonstrating detailed workflow decomposition for this domain.
3. **Meta-Development Insights:** We describe the **meta-prompting** and **meta-reasoning** techniques used to iteratively develop and refine the PWP prompt, offering practical insights applicable to creating other complex, structured prompts.
4. **Empirical Demonstration:** We provide a qualitative **demonstration** and **analysis** of the PWP prompt's application using readily available reasoning LLMs, showcasing its ability to generate detailed, structured peer-review-like feedback incorporating multimodal analysis and quantitative checks.

The remainder of this paper is organized as follows: Section 2 details the methodology, including the **meta-prompting techniques** used (2.1), the **architecture of the PWP prompt** (2.2), and the **process of formalizing the review workflow** (2.3). Section 3 discusses the **results based on the demonstration analyses** (3.1), outlines **study limitations** (3.2), and **considers further development** (3.3).

### 2.3. Formalizing the Review Process

#### 2.3.1. Translating Expert Review into Actionable Prompts

A significant challenge in developing AI systems for tasks like scholarly peer review lies in translating the complex, often nuanced, reasoning processes of human experts into explicit, executable instructions suitable for an LLM. Expert review relies heavily on domain-specific knowledge, critical thinking, pattern recognition, and a considerable amount of tacit knowledge [[74]] - intuitions, heuristics, and ingrained understandings that experts apply subconsciously and often find difficult to articulate fully. Consequently, simply asking an LLM to "review a paper" typically yields superficial results, lacking the depth and critical rigor of true expert evaluation.

This limitation stems partly from the nature of generative pre-trained models. By default, LLMs often process input text by integrating it with their existing knowledge base, excelling at tasks like summarization where the input is largely taken at face value. Critical analysis, however, requires a different stance - one of abstraction and skepticism, where the input manuscript is evaluated against external principles and knowledge without being automatically accepted as truth. This critical stance, treating the manuscript as an object of scrutiny rather than incorporated fact, is generally not the default behavior and requires specific guidance. While frontier LLMs can perform complex abstract operations, eliciting in-depth critical analysis necessitates either specialized training or, as explored in this work, advanced prompting techniques designed to guide the model through a rigorous, structured evaluation process.

Therefore, creating the _PeerReviewPrompt_ necessitated a deliberate process of formalizing the intellectual workflow of critical review in experimental chemistry, aiming to make the implicit explicit and codify expert reasoning into a structured, actionable protocol. The subsequent sections detail this formalization process, including reflections on the meta-reasoning involved.

### 3.1 Demonstration Analyses

The _PeerReviewPrompt_ was primarily developed using Google Gemini Advanced 2.5 Pro, with earlier exploration involving ChatGPT Plus o1. Demonstration analyses of the _test paper_ [[91]] (including its SI) for several frontier reasoning models driven by this prompt are included in appendixes and linked in **Supporting Information** (**Gemini Advanced 2.5 Pro**, **ChatGPT Plus o3**, **ChatGPT Plus o1** [[92]], and **SuperGrok Grok 3 Think** [[93]).

As expected, the specific details and phrasing of the analyses varied between models and even between different runs on the same model. However, a key observation was the consistency in identifying core issues: all tested models, when guided by the _PeerReviewPrompt_, relatively reliably identified major methodological flaws within the _test paper_ [[91]] and converged on the conclusion that its central claim (regarding isotopic enrichment) was highly dubious or unsupported by the described methods. This consistency across different architectures suggests the structured workflow provided by PWP effectively directs LLM reasoning towards critical evaluation points.

Early tests with early versions of the prompt demonstrated that, when performing manuscript analysis and driven by simpler prompts, LLMs behave as if they were positively biased. For example, an LLM could question soundness of overall highly improvised experimental setup or its specific aspects, but then it would concluded that achievement of high enrichment in the final product indicated that the experimental apparatus successfully worked. This behavior, which could also be interpreted as outcome bias discussed in persona engineering **Section 2.2.3**, might be rationalized when considered the modern LLMs' in-context learning (ICL) ability.

The simplest demonstration of in-context learning can be illustrated via the few-shot prompting technique, where the model is provided several examples of input/output pairs and is asked to generate output for input that is somehow similar to the inputs included in reference examples. More generally, ICL is demonstrated when the user engages in a conversation with an LLM. ICL ability is manifested as progressively more relevant responses that are shaped by the prior discussion. Further, with introduction of persistent context / memory (as opposed to temporary context that incorporates only inputs provided with the current chat or session), LLMs become available also take into account conversational history that occurred in prior conversations / sessions. This is a very important feature that makes it possible for LLMs to compensate at least to certain degree the limitations of their training (such as knowledge cutoff date or lacking training data). At the same time, to be able to learn, LLMs must be able to accept the inputs as learning material. Simultaneous ability to use input material for learning, but also critically evaluating it is a difficult task requiring abstract reasoning. This problem, in fact, may be compared, for example, to learning in children. Generally still lacking advanced critical thinking skills or having only rudimentary abilities, the dominating mode of learning in children involves accepting the learning material they are provided as established ground truth, that is learning material is taken at its face value without questioning its truthfulness (also mentioned in **Section 2.3.1**). Whether LLMs can be trained or fine-tuned for more critical evaluation of inputs, while still demonstrating robust ICL, is a separate question. Overall, present experience is that the default behavior of tested frontier models via LLM chat bots is akin to that of children, and eliciting critical treatment of input material necessitates prompt-driven context conditioning. Countering apparent outcome bias (but what most likely effective _positive input bias_) and pushing it towards negative bias (see **2.3.4**), was, therefore, one of the essential goals of such context conditioning via persona engineering discussed in **Section 2.2.3**. With tested models and the _test paper_, _negative input bias_ context conditioning present in the current _PeerReviewPrompt_, successfully and reliably suppressed _positive input bias_.

---
### 3.2. Demonstration Findings: Multimodal Capabilities and Augmented Review

A noteworthy aspect highlighted by the demonstrations relates to multimodal analysis capabilities. For example, Google Gemini Advanced 2.5 Pro (the subscription-based version) repeatedly demonstrated ability to analyze image content (specifically, photograph in SI Figure 1 of the _test paper_ [[91]]) and integrate information extracted from visuals with the textual context, as guided by the PeerReviewPrompt. For instance, it consistently identified the presence of aluminum foil insulation around the fractionation column depicted - a detail absent from the main text. Furthermore, following prompt instructions, it successfully inferred approximate scale information from main text and applied this inferred data to subsequent steps involving the analysis of physical processes. While OpenAI has also indicated multimodal capabilities for its recent o3 reasoning model [[22, 94]], the limited testing performed during this work did not yield convincing evidence of integrated visual-textual analysis for this specific task. Furthermore, verifying the extent of such capabilities in ChatGPT models can be challenging due to the lack of transparency regarding their internal reasoning or step-by-step thought processes compared to models like Gemini Advanced.

Secondly, several runs flagged inconsistencies related to the boiling points (b.p.) reported for different fractions (Table 1 in [[91]]). Although the prompt did not specifically target b.p. analysis (potentially explaining why this issue was not consistently flagged), the observation prompted closer scrutiny. Comparing the differences in reported uncorrected b.p. values between fractions reveals discrepancies when contrasted with known literature values for H216O, H217O, and H218O (which span only ~0.2°C at 1 atm according to [[95]], Table 9.1). This observation, combined with the authors' failure to monitor or report ambient pressure despite claiming a significant (10-15 times higher than the b.p. span of separated components) altitude-based b.p. depression for tap water, raises further critical questions regarding the meaning of the reported data and the entire study.  This particular issue was initially missed by human review but surfaced by several PWP-guided LLM analysis runs.


Intriguingly, the LLM analyses highlighted at least two potentially significant issues not initially noted by the author during manual review. Firstly, multiple models consistently identified the use of a glass-wool-packed condenser as an improvised fractionating column as a poor methodological choice likely insufficient for the claimed separation. The models also usually suggested conventional accessible alternatives with potentially significantly higher and well-characterized performance. While evaluating this specific detail falls outside the author's direct expertise, the consensus across models and preliminary external checks suggest this criticism is likely valid. 

These observations suggest the potential for PWP-guided LLMs not only to structure analysis but also to augment human review by identifying flaws that might be overlooked due to differing expertise or attention patterns. However, these findings are preliminary. A systematic comparison of analyses across models and multiple runs, potentially using quantitative metrics alongside qualitative assessment, is required for a rigorous evaluation of the prompt's performance, reliability, and limitations. Such a detailed comparative analysis was beyond the scope of this initial proof-of-concept study.

