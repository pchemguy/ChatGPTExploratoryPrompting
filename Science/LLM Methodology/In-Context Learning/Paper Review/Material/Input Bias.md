Here are full relevant sections to work on. Presently, I see three main tasks (all of which appear to be of broad importance that spans far beyond the present project's scope) I encountered when started this project:
1. Tacit expert knowledge codification
2. Development of generalized workflow for prompt that would guide AI analysis.
3. Countering input biases
While current intro identifies several difficulties that should be mentioned, there is no articulated mentioning of any of the tasks above. In fact, perhaps there is some linking to be made. The complexity of the problem, lack of training data and limitations of existing approaches discussed in the second half of the second para of 1.1 can be linked to the necessity to develop detailed workflow (task 2). Need for field specific tailoring - expert knowledge codification (task 1). And I do not discuss the input biases in 1.1 at all, as I only recently articulated this matter.

---
INTO NOTES FOR INPUT BIAS

Input bias 

---

### 1.1. Scholarly Peer Review

Scholarly peer review is a cornerstone of academic research, demanding significant time, domain expertise, and critical reasoning. Using technical means to facilitate this process is a long-standing goal, which has gained urgency with the explosive growth of publishing activities and the recent advances in generative AI technologies increasingly used in academic publishing [[36, 37]]. The last few years alone have witnessed a wealth of publications addressing this automation problem via diverse approaches, including basic and methodological research [[38–46]], graph-based manuscript modeling [[47]], prompt-focused techniques [[42, 46]], probing capabilities of private and open-source models [[38, 48, 49]], investigations with reasoning models [[40, 50]], training custom models [[49–51]], developing multi-model/agentic systems [[38, 47–49, 52, 53]], and launching publicly accessible services [[38, 43, 50, 54]]. Due to its intellectually demanding nature, using AI for peer-review-like feedback also serves as a valuable method for evaluating and pushing the boundaries of advanced models.

Despite this progress, automating peer review remains a significant challenge for modern AI [[39–41, 55]]. Key difficulties include inherent complexity of this task, the need for field-specific tailoring, and the historical lack of readily available, large-scale training datasets (with a number of attempts to address the latter issue [[43, 45, 48–50, 56–63]]). Furthermore, existing approaches often face limitations. Training data consisting of high-level reviewer comments may not effectively teach models the detailed, step-by-step reasoning required for rigorous manuscript evaluation. Similarly, prompts based solely on common reviewer guideline questions (e.g., [[64]]) may fail to elicit the necessary depth of analysis compared to methods like chain-of-thought (CoT) prompting [[65–67]].

#### 2.3.1. Translating Expert Review into Actionable Prompts

A significant challenge in developing AI systems for tasks like scholarly peer review lies in translating the complex, often nuanced, reasoning processes of human experts into explicit, executable instructions suitable for an LLM. Expert review relies heavily on domain-specific knowledge, critical thinking, pattern recognition, and a considerable amount of tacit knowledge [[74]] - intuitions, heuristics, and ingrained understandings that experts apply subconsciously and often find difficult to articulate fully. Consequently, simply asking an LLM to "review a paper" typically yields superficial results, lacking the depth and critical rigor of true expert evaluation.

This limitation stems partly from the nature of generative pre-trained models. By default, LLMs often process input text by integrating it with their existing knowledge base, excelling at tasks like summarization where the input is largely taken at face value. Critical analysis, however, requires a different stance - one of abstraction and skepticism, where the input manuscript is evaluated against external principles and knowledge without being automatically accepted as truth. This critical stance, treating the manuscript as an object of scrutiny rather than incorporated fact, is generally not the default behavior and requires specific guidance. While frontier LLMs can perform complex abstract operations, eliciting in-depth critical analysis necessitates either specialized training or, as explored in this work, advanced prompting techniques designed to guide the model through a rigorous, structured evaluation process.

Therefore, creating the _PeerReviewPrompt_ necessitated a deliberate process of formalizing the intellectual workflow of critical review in experimental chemistry, aiming to make the implicit explicit and codify expert reasoning into a structured, actionable protocol. The subsequent sections detail this formalization process, including reflections on the meta-reasoning involved.

### 3.1 Demonstration Analyses

The _PeerReviewPrompt_ was primarily developed using Google Gemini Advanced 2.5 Pro, with earlier exploration involving ChatGPT Plus o1. Demonstration analyses of the _test paper_ [[91]] (including its SI) for several frontier reasoning models driven by this prompt are included in appendixes and linked in **Supporting Information** (**Gemini Advanced 2.5 Pro**, **ChatGPT Plus o3**, **ChatGPT Plus o1** [[92]], and **SuperGrok Grok 3 Think** [[93]).

As expected, the specific details and phrasing of the analyses varied between models and even between different runs on the same model. However, a key observation was the consistency in identifying core issues: all tested models, when guided by the _PeerReviewPrompt_, relatively reliably identified major methodological flaws within the _test paper_ [[91]] and converged on the conclusion that its central claim (regarding isotopic enrichment) was highly dubious or unsupported by the described methods. This consistency across different architectures suggests the structured workflow provided by PWP effectively directs LLM reasoning towards critical evaluation points.

Early tests with early versions of the prompt demonstrated that, when performing manuscript analysis and driven by simpler prompts, LLMs behave as if they were positively biased. For example, an LLM could question soundness of overall highly improvised experimental setup or its specific aspects, but then it would concluded that achievement of high enrichment in the final product indicated that the experimental apparatus successfully worked. This behavior, which could also be interpreted as outcome bias discussed in persona engineering **Section 2.2.3**, might be rationalized when considered the modern LLMs' in-context learning (ICL) ability.

The simplest demonstration of in-context learning can be illustrated via the few-shot prompting technique, where the model is provided several examples of input/output pairs and is asked to generate output for input that is somehow similar to the inputs included in reference examples. More generally, ICL is demonstrated when the user engages in a conversation with an LLM. ICL ability is manifested as progressively more relevant responses that are shaped by the prior discussion. Further, with introduction of persistent context / memory (as opposed to temporary context that incorporates only inputs provided with the current chat or session), LLMs become available also take into account conversational history that occurred in prior conversations / sessions. This is a very important feature that makes it possible for LLMs to compensate at least to certain degree the limitations of their training (such as knowledge cutoff date or lacking training data). At the same time, to be able to learn, LLMs must be able to accept the inputs as learning material. Simultaneous ability to use input material for learning, but also critically evaluating it is a difficult task requiring abstract reasoning. This problem, in fact, may be compared, for example, to learning in children. Generally still lacking advanced critical thinking skills or having only rudimentary abilities, the dominating mode of learning in children involves accepting the learning material they are provided as established ground truth, that is learning material is taken at its face value without questioning its truthfulness (also mentioned in **Section 2.3.1**). Whether LLMs can be trained or fine-tuned for more critical evaluation of inputs, while still demonstrating robust ICL, is a separate question. Overall, present experience is that the default behavior of tested frontier models via LLM chat bots is akin to that of children, and eliciting critical treatment of input material necessitates prompt-driven context conditioning. Countering apparent outcome bias (but what most likely effective _positive input bias_) and pushing it towards negative bias (see **2.3.4**), was, therefore, one of the essential goals of such context conditioning via persona engineering discussed in **Section 2.2.3**. With tested models and the _test paper_, _negative input bias_ context conditioning present in the current _PeerReviewPrompt_, successfully and reliably suppressed _positive input bias_.

A noteworthy aspect highlighted by the demonstrations relates to multimodal analysis capabilities. For example, Google Gemini Advanced 2.5 Pro (the subscription-based version) repeatedly demonstrated ability to analyze image content (specifically, photograph in SI Figure 1 of the _test paper_ [[91]]) and integrate information extracted from visuals with the textual context, as guided by the PeerReviewPrompt. For instance, it consistently identified the presence of aluminum foil insulation around the fractionation column depicted - a detail absent from the main text. Furthermore, following prompt instructions, it successfully inferred approximate scale information from main text and applied this inferred data to subsequent steps involving the analysis of physical processes. While OpenAI has also indicated multimodal capabilities for its recent o3 reasoning model [[22, 94]], the limited testing performed during this work did not yield convincing evidence of integrated visual-textual analysis for this specific task. Furthermore, verifying the extent of such capabilities in ChatGPT models can be challenging due to the lack of transparency regarding their internal reasoning or step-by-step thought processes compared to models like Gemini Advanced.

Intriguingly, the LLM analyses highlighted at least two potentially significant issues not initially noted by the author during manual review. Firstly, multiple models consistently identified the use of a glass-wool-packed condenser as an improvised fractionating column as a poor methodological choice likely insufficient for the claimed separation. The models also usually suggested conventional accessible alternatives with potentially significantly higher and well-characterized performance. While evaluating this specific detail falls outside the author's direct expertise, the consensus across models and preliminary external checks suggest this criticism is likely valid. Secondly, several runs flagged inconsistencies related to the boiling points (b.p.) reported for different fractions (Table 1 in [[91]]). Although the prompt did not specifically target b.p. analysis (potentially explaining why this issue was not consistently flagged), the observation prompted closer scrutiny. Comparing the differences in reported uncorrected b.p. values between fractions reveals discrepancies when contrasted with known literature values for H216O, H217O, and H218O (which span only ~0.2°C at 1 atm according to [[95]], Table 9.1). This observation, combined with the authors' failure to monitor or report ambient pressure despite claiming a significant (10-15 times higher than the b.p. span of separated components) altitude-based b.p. depression for tap water, raises further critical questions regarding the meaning of the reported data and the entire study.  This particular issue was initially missed by human review but surfaced by several PWP-guided LLM analysis runs.

These observations suggest the potential for PWP-guided LLMs not only to structure analysis but also to augment human review by identifying flaws that might be overlooked due to differing expertise or attention patterns. However, these findings are preliminary. A systematic comparison of analyses across models and multiple runs, potentially using quantitative metrics alongside qualitative assessment, is required for a rigorous evaluation of the prompt's performance, reliability, and limitations. Such a detailed comparative analysis was beyond the scope of this initial proof-of-concept study.

