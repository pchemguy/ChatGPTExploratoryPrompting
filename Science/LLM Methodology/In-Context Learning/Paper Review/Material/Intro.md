## Introduction: An AI Prompt for Simulating Academic Peer Review

The following document details the design and rationale behind a specialized AI prompt engineered to simulate the rigorous process of academic peer review, with a particular focus on experimental chemistry manuscripts. While Large Language Models (LLMs) have shown remarkable capabilities, performing nuanced, domain-specific tasks like critical scientific evaluation remains a significant challenge. Standard LLMs often lack the specialized knowledge, established workflows, and the ability to execute complex, multi-step reasoning required for a thorough peer review.

This prompt addresses these limitations by employing advanced prompt engineering techniques, primarily relying on **in-context learning**. It provides the LLM with a comprehensive framework—including detailed instructions, analytical workflows, specific knowledge points, and even a defined reviewer persona—directly within the prompt context. This allows the model to simulate a structured, critical assessment process without needing specialized fine-tuning or external tools.

We will explore the technical aspects of LLMs relevant to this approach, such as context windows and different types of context (input, transformation, output, behavioral). We will then delve into the specific design methodology behind this prompt, including its hierarchical structure, use of Markdown for clarity, multimodal analysis capabilities, and strategies for encouraging critical thinking and quantitative feasibility checks, aiming to replicate key aspects of expert human review.