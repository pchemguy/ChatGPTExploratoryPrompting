Perhaps, I should also mention, that the workflow attempts specifically define sufficiency check for the action (verb + modifiers) and concept / scope modifiers (syntactic object, including modifiers). Action is often linked in a less direct fashion to antecedent, like in case of interpretive claims (illustrates, demonstrates, suggests, etc.).

In fact, whether the action is properly supported may be beyond the scope of pronoun ambiguity check. It does not matter whether pronoun is used when rereferring to routinely mentioned technique as "demonstration of its power". This might be a good candidate for separate semantic check irrespective of the vague pronoun use. On the other hand, the use of standalone "this" may often interfere with writer's own ability to understand and see issue in their writing. For example, if the author of this text:

~~~

Five other 17O-labeled compounds were also prepared from the 17O-labeled water (...) and characterized by NMR and GC-MS. This illustrates the power of 17O NMR in the detection of the reactions of O-containing functional groups.

~~~

would attempt to disambiguate "this" properly, they perhaps would see that a clear subject, appropriate for the routine non-specific mentioning of NMR (it does not even specify the kind of NMR, even though this detail can be inferred if the entire paper is considered) would look very odd when combined with "illustrates the power of 17O NMR" even without the more detailed scope.

The head noun of the object, "power" is also fairly abstract consistent with interpretive claim, and even with its essential first level modifier "power of 17O NMR" may not be a straightforward task for the AI to judge, whether the antecedent appropriately supports it. This pattern, however, might be another specific, but a fairly common abstract pattern, present in conclusions (interpretive claims with pronoun must refer to local context, but, generally, these kind of claims should have one of the important results of the study, as its implied antecedent, not a routinely used/mentioned standard technique).

But the rest of the syntactic object, particularly the modifiers, should probably be more concrete and more directly present in the antecedent context. Hence, the attempt to explicitly separate "substantive components" and "Action/Verb" in the prompt workflow.

### 4.1. Informational Integrity Analysis

The first  test, `Informational Integrity Analysis`, targeted two identified numerical quantiles introduced in the Conclusions section of the test case (unsubstantiated information), as discussed in the ground truth analysis above, using the prompt *"ConclusionsClassificationAndReferencesPrompt.md"*. The tested Gemini model successfully identified both targets at the success rate of 95% (Table 1). Curiously, the ChatGPT model demonstrated near binary performance: it flagged one of the quantities with 95% success rate, while completely missing (0% success rate) the other one (Table 1). When these result were interpreted by Gemini, the model made an interesting observation. Syntactically, "90 mL" (correctly flagged by both models) is the head of a noun phrase that acts as the subject. On the other hand, "40-fold" (completely missed by ChatGPT) functions as an adjectival modifier. It possible that the latter more "obscure" role contributed to ChatGPT failure.

This test involves a number of challenges. First, the model needs to analyze the entire manuscript, with summary section potentially buried in the middle of a large document. While modern models demonstrate robust analytical capabilities with respect to the structure of conventional technical texts, non-conventional structure, headings, or even combined sections may complicate the process of identification and extraction of a summary section, particularly Conclusions. Further, within the summary, the model needs to accurately isolate every "atomic" claim (information unit, IU) with appropriate context. These requirements necessitate sufficiently large context window, accurate recall, and strong analytical capabilities for splitting the target section. Locating specific sources of individual identified IUs may not be a trivial task either. Numeric quantities, including derived values, often should appear literally in the appropriate IMRaD sections. At the same time, rounded values or ranges might be more appropriate in a summarizing statement. Non-numeric claims, e.g., related to methods or materials (not tested in this work), where appropriate, may use synonyms or semantically equivalent expressions, further stressing LLM's "reasoning" abilities. However, it is reasonable to expect that the challenges associated with locating specific non-trivial sources will more likely result in false positives, meaning a potential non-issue case being brought to human's attention rather than failing to flag a real problem.

To facilitate development process and potentially improve robustness of prompts, two major approaches have been employed, including development of modular workflow structure and of a custom Classification System for Information Units. Custom classification system may facilitate extraction of IUs from unstructured text and their subsequent analysis, though no attempt has been made in this study to test this hypothesis quantitatively or qualitatively. Decomposition techniques, techniques like CoT (and its variations), as well as our previous work [8] provide strong evidence in favor of workflow prompts design for complex analytical tasks. While both non-reasoning and reasoning models demonstrate improved performance with such techniques, reasoning models possess naturally better capability to take advantage of complex workflow prompts.

Modular structured prompt design potentially offers a number of benefits. In present case, the prompt employs a linear workflow process that "drills down" the hierarchical organization of technical texts, starting from the entire input and gradually narrowing focus in a staged process eventually zeroing in on "atomic" IUs. The idea here is that any intermediate result/output (particularly for reasoning models) becomes part of models context that can be used/addressed in subsequent steps. For example, once the Conclusions section is identified and extracted, it becomes a separate "focused" piece in the models context, and subsequent stages involving this section, in a sense, no longer need to address the entire manuscript (though in present case, no attempt to gauge potential performance improvement due to the detailed workflow prompt design has been made). Modular structured workflow design offers two other important benefits. On the one hand, prompt sections can be reused in different prompt involving the same sub-tasks, such as identification of a summary section and splitting it into sentences, as demonstrated by the two prompts developed for the two reported test targets. Equally important, is the structured prompt's (with detailed output) potential ability to provide insights into LLM's specific failure modes.

Close examination of detailed individual LLM responses revealed a number of observations.
- **Instruction Following - Report Output:**
  ChatGPT's outputs were generally considerably more terse, often ignoring some of the instructions. For example, the subsection **"5. Report Verification Findings for Each Information Unit (IU)"** of **"Phase 5: Referencing and Verification of Information Units (IU)"** specifically mandated to provide details on both substantiated (**Subsection A**) and unsubstantiated (**Subsection B**) items. Additionally, "**4. Evaluate Numeric Quantities (If the IU contains them)**" places specific focus on numeric quantities. ChatGPT output largely ignored these instructions, as opposed to output of Gemini, making it difficult to diagnose potential issues.
   
  Gemini's output was generally more insightful. For example, the only case classified as failure to flag the "40-fold" piece clearly reveals that Gemini successfully extracted this quantity (like in all other cases), considered it, and provided an explanation of its non-flagging decision: `"40-fold enriched water" is a reasonable summary of the pre-concentration by evaporation step.` Similarly, with the other case: `The "about 90 mL" output is reasonably close to the sum of the most enriched fractions detailed (70 mL).`
- **No False Positives:**
  Neither model flagged any other numeric quantity, which is the desired result according to our ground truth analysis. Gemini flagged several times evaluative/interpretive statements, such as:
  
  `The assertion that "The most practical method for determining the enrichment was found to be the reaction of the H217O with BSTFA to yield hexamethyldisiloxane". While the method is described, its comparative "practicality" over other determination methods detailed (e.g., 1-hexanol derivatization) is not explicitly justified or stated as a finding within the IMRaD sections.`
  
  These targets are more difficult/abstract and were not targeted by the present version of the prompt, though it might be conceivable to target such targets with crafted instructions. 

### 4.2. Linguistic Clarity Analysis

The other  test, `Linguistic Clarity Analysis`, targeted a problematic standalone "this" reference, as discussed in the ground truth analysis above, using the prompt "ConclusionsLinguisticAnalysisPrompt.md". The context for this analysis is limited solely to the summary section being analyzed, that is any "this/it" reference must have a clear antecedent within the section itself. For this reason, this analysis could be implemented in two modes: "limited context", with only the Conclusions section provided to the LLM, and "full context", where model would need identify the Conclusions section in the full manuscript. ChatGPT model demonstrated consistently high success rate (according to criteria discussed in the Results section) in both modes, with 85% rate with in the full context mode and reaching a perfect score in the limited context mode (Table 2). Gemini model demonstrated the same success rate with full context, but surprisingly low performance in the limited context mode. Gemini also demonstrated somewhat apparently odd behavior during the first Series A tests, which were excluded from further consideration (Table 3).

The oddity with Series A Gemini analysis was in distribution of failed runs. These two series involved 41 runs total executed in succession. Oddly, in both cases the majority of failed runs were in the last half of the corresponding series. For this reason, both series were doubled in length (40 runs each) and repeated twice on two separate days (series B and C). The series B and C demonstrated consistent behavior:  full context - close high performance (matching that of the ChatGPT model); limited context - consistently low performance.

As discussed in the Results section the present prompt version primarily targeted the concrete "detection of reactions" scope modifier. Close examination of models' output revealed, however, that both models also correctly flagged the abstract concept "power of 17O NMR" as unsupported by the antecedent at nearly identical rate. In the case of poorly preforming limited-context series with Gemini, flagging of "power of 17O NMR" was also comparably poor.

