# **Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging No New Information and Ambiguous Pronouns**

## 1. **Introduction**

While research into computer-assisted (and, more recently, AI-assisted) tools for academic writing has a long history \[1\], rapidly evolving state-of-the-art (SOTA) general-purpose large language models (LLM) (such as Gemini Pro 2.5 Pro [2], ChatGPT Plus o3 [3], and Claude Opus 4 [4]) enable accessible semantic and linguistic analysis \[5, 6\] and synthesis \[7\] of technical texts. In particular, sufficiently large context window length (a.k.a., input token limit) enables analysis of full-length research papers together with supporting information \[8, 9\] and considerably expands the power of specialized prompt engineering techniques \[10\]. Furthermore, by taking advantage of the in-context learning (ICL) \[11–13\] capabilities, advanced prompting strategies \[14–22\] (such as chain-of-thought (CoT) \[23–25\], least-to-most prompting \[26\], task decomposition \[27–29\], and role-playing \[30, 31\]), and formalized expert domain knowledge sophisticated pure-prompt-based (no API access or coding required) guidance makes it possible to systematically focus model's attention on a broad spectrum of specialized aspects of a technical text in a generalizable way yielding highly insightful analyses \[8, 9\].

An important example of prompt-based LLM model guidance includes instructions and clues focusing on structure of technical text. In particular, many scholarly publications reporting original research follow the common IMRaD (Introduction, Methods, Results, and Discussion) \[32\] structure. Two additional essential sections, not included in this abbreviation, are academic summaries, Abstract and Conclusions, coming before and after the IMRaD body (main text), respectively. Assessment of semantic and linguistic quality of Abstract and Conclusions is the objective of this study.

The specific issues, targeted by this work, were selected based on problems identified in the test paper \[33\], used in out previous work \[8, 9\], namely unsubstantiated facts and ambiguous pronouns that may be appear in summaries. Summaries generally serve the purpose of highlighting key features of the reported work for the target audience. All factual information found in summaries is understood to originate from, and be substantiated by, the main IMRaD (Introduction, Methods, Results, Discussion) content of the paper. If a result, a derived quantity, or an observation is present in a summary section, but not in the main IMRaD content, such information should usually be flagged. Another common (linguistic) issue involves the use of ambiguous pronouns, which may disrupt text flow and complicate comprehension. To diagnose these issues, we developed several proof-of-concept prompts, included in appendixes and supporting information. We have share interactive AI chats demonstrating the prompt development process.

## 2. **Methodology**

### 2.1 **Test Case**

This study uses a deliberately selected test publication \[33\], focusing on its Abstract and Conclusions sections. The test paper file (also available via a link in [Supporting Information](#bookmark=id.in0f7efcmy17)) constitutes a combination of the main text and supporting information files (as available via paper's DOI \[33\]). This is the same test file also used in our prior works for analysis of the paper's core methodology \[8\] and identifying chemical formula issues \[9\], which is why the file still includes Supporting Information, not used in this study. This time, we focus on the summary sections \- Abstract and Conclusions: we identify common issues within these sections and craft prompts to flag such issues.

The first issue identified involves introduction of new information not discussed in the main sections. Because summaries generally serve to highlight key features of the study described in detail in the main text, it is generally expected that these summaries should not introduce any new information. In fact, there is a good rationale for this convention. As summaries are meant to provide concise focused highlights, they are not place for providing context linking the highlighted information to other parts of the study and interpreting this information, which are essential for presenting academic results. On the other hand, any attempt to provide satisfactory context directly in a summary section would most likely immediately and negatively affect its primary role. Therefore, any highlighted numerical result (including quantities that can be derived from the raw data included in the main sections), observation, or methodological approach must be described in detail in the appropriate main section.

Specifically, the test paper reports the use of a two-stage process (simple natural evaporation followed by fraction distillation) for isotopic 17O enrichment of natural abundance water. While the Abstract section generally follows the "no new information" rule, the third sentence of Conclusions states: "From approximately **500 mL of** **40-fold enriched water, about 90 mL of H217O** was obtained." This sentence follows the second sentence that specifically refers to the second fractional distillation stage, meaning the stated input of "500 mL" (found in the main text) of "40-fold enriched water" (not found in the main text) refers to water collected after the first evaporation stage. The "90 mL of H217O" bit cannot be found in the main text either. The specific phrasing may be consistent with this quantity being derived from the results **Table 1**, but data in that table does not support this quantity either.

Another interesting and challenging test target involves a common case of the naked/standalone use of "this" (and, perhaps, less frequent plural "these") determiner. "The way that many scientists and engineers treat the pronoun "it" is unsettling but the way that many scientists and engineers treat the word 'this' is criminal ... Worse yet, many of those uses \[of 'this'\] refer to different things: the last noun used, the subject of the previous sentence, the idea of the previous sentence, or something else." \[34\]. Because of this inherent ambiguity, it might be often easier and tempting for the writer to simply use the universal "this", rather then fully develop and articulate precise intended meaning. By using essentially sloppy language, however, the writer (by oversight or sometimes intentionally) shifts the burden of elucidating the writer's actual meaning to the reader \- an unacceptable practice in science and engineering fields.

The Conclusions' second sentence from the end states: "**This** illustrates the **power of 17O NMR** in the **detection of the reactions** of O-containing functional groups." Further, while constructs like "this paper" or "this study" are standard, clear, and refer to the overarching context (beyond the local context) defined by the noun, a standalone "this" should refer to its antecedent present in the local context (in this case, Conclusions) just **prior** the actual reference. In present case, the preceding sentence (starting with "Five other 17O-labeled compounds") does mention "NMR", but clearly in an unrelated context (no references to any *reactions detected by NMR*. The last sentence does mention a *reaction detected by NMR*, which could be a satisfactory antecedent, if it had preceded the "this" reference. We have to conclude, therefore, that the "this" reference in question not only ambiguous, but does not have a satisfactory antecedent either, and would like to have LLM to flag it clearly as such.

### 2.2 **Prompting and Prompt Development Strategy**

Development of prompts for "no new information" and "vague/ambiguous pronouns" analyses and subsequent tests were performed using the *Gemini Pro 2.5 Pro* model via the official chat bot web interface. From the very beginning, we aimed to develop a workflow-style prompts that decompose the overall task into simpler better defined subtasks. The development process proceeded in several stages.

1. **Basic interactive step-by-step analysis:**

* Decomposing the task into subtask candidates.  
* Drafting basic prompts for each identified subtask.  
* Performing interactive step-by-step analysis of the test case by submitting candidate subtask prompts one at a time within the same conversation.  
* Evaluating of LLM responses with the goal to identify major issues in subtask prompt instructions and decomposition scheme, iterating on the first two points, if necessary.

2. **Development of workflow prompts:**

Development of the target prompt was performed via meta-prompting techniques \[8\] using the "Adaptive Prompt Engineering Assistant & Tutor" prompt (\[8\], Appendix B, *Prompt\_Engineer\_Peer\_V2.md*). Intermediate prompt drafts were frequently tested against the test case to ensure that instructions for each subtask produced acceptable result before moving on to implementation of the next subtask.

3. **Iterative refinement of workflow prompts:**

Initially developed full prompt drafts were used to perform the test case analysis and subsequent prompt refinement, when necessary.

## 3. **Results and Discussion**

The general analysis workflow was modeled after a process a human could have attempted if presented with a similar analytical task. Proposed steps were first tested interactively via Gemini Pro 2.5 Pro, as illustrated by this shared AI chat; then several specialized workflow prompts have been developed iteratively (in this shared AI chat), including "no new information" check (see Appendix and SI) and "ambiguous pronouns" check (see Appendix and SI) for Conclusions. Similar prompts for Abstract are also provided as SI.

### 3.1. **No New Information Check**

As the task structure of the prompt indicates, the "no new information" check was split into five stages/phases. The first stage \- identification and extraction of the conclusions section. This part may be tricky if the conclusion section uses unconventional name or not included as a separate section at all. In such a case, the AI is instructed to clearly state the problem and terminate analysis. The second stage asks AI to produce the conclusions section so that the user could verify correctness of the first stage, following by splitting conclusions into individual sentences. Often, each summary sentence may contain more than one information unit (IU, such as, specific result, observation, or name of a key method/technique used). Initial attempts to craft instructions for reliable splitting into such units proved tricky, so a decision was made to perform the sentence-level analysis in a separate stage. The third stage seeks a balanced splitting of individual sentences into IUs while avoiding creation of meaningless chunks. This stage is not considered essential, so even though some variability between runs remained, the result was deemed acceptable for a proof-of-concept. The fourth non-essential stage asks AI to classify identified IUs according to a custom Classification System for Information Units. The final stage in this workflow instructs AI to find sources of identified IUs in the IMRaD body and clearly identify such sources or indicate that no appropriate source has been found.

The custom Classification System for Information Units was developed to further explore the idea mentioned in our previous work \[8\] focused on use of custom classification system as a means to facilitate LLM-based semantic analysis of material extracted from a manuscript text. Because Abstract and Conclusions are both summaries, though with somewhat different focus, Classification System was designed to include both common categories as well as categories more typical to either Abstract or Conclusions. This way, the Classification system could be used as a modular unit that could be directly copied between different prompts tailored to both Abstract and Conclusions. Each item of this system contains a brief description of the category, typical IMRaD section, where such items should be introduced, and additional notes to LLM. In this work, this classification system was solely used to tell LLM which IMRaD section it should focus on when looking for IU source. This system could potentially also used in synthetic workflows designed to extract candidate chunks from the IMRaD body for inclusion into abstract and conclusions (that is, AI-assisted generation of abstract and conclusions). While initial attempts to perform such extraction was performed, this subject is beyond the scope of present work.

### 3.2. **Ambiguous Pronouns Analysis**

While LLMs appear to be fairly robust with respect to identifying and flagging (and fixing, if requested) ambiguous/vague pronouns, correctly flagging an instance that does not have an acceptable antecedent might be trickier. Early tests demonstrated that the LLM (Gemini Pro 2.5 Pro) in the present tricky case \- "**This** illustrates the **power of 17O NMR** in the **detection of the reactions** of O-containing functional groups." (the sixth sentence of Conclusions, the second from the end) \- would often label either the sentence that immediately preceded the target "this" (the sentence that mentioned NMR in an unrelated context) or the last sentence that followed the target "this". To improve robustness of the prompt, I indicated the failure to the LLM in the prompt development conversation and asked it to improve the prompt (search the shared AI chat for LLM response containing "Suggested Revised Section for Your Prompt"). The generated modified prompt repeatedly demonstrated its ability to correctly flag the test case in question.